<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Columbia DSI Scholars</title>
    <link>/tags/python/</link>
    <description>Recent content in Python on Columbia DSI Scholars</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Sep 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Cryptocurrency Analytics: Identifying Bad Actors</title>
      <link>/2019/09/project-cryptocurrency-analytics-identifying-bad-actors/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/project-cryptocurrency-analytics-identifying-bad-actors/</guid>
      <description>&lt;p&gt;Many of the cryptocurrency transactions have involved fraudulent activities including ponzi schemes, ransomware as well money-laundering. The objective is to use Graph Machine Learning methods to identify the miscreants on Bitcoin and Etherium Networks. There are many challenges including the amount of data in 100s of Gigabytes, creation and scalability of algorithms.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Decoding the human genome with interpretable deep learning</title>
      <link>/2019/09/project-decoding-the-human-genome-with-interpretable-deep-learning/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/project-decoding-the-human-genome-with-interpretable-deep-learning/</guid>
      <description>&lt;p&gt;The function for much of the 3 billion letters in the human genome remain to be understood. Advances in DNA sequencing technology have generated enormous amount of data, yet we don&amp;rsquo;t have the tool to extract rules of how the genome works. Deep learning holds great potential in decoding the genome, in particular due to the digital nature of DNA sequences and the ability to handle large data sets. However, like many other applications, the interpretability of deep learning models hampers its ability to help understand the genome. We are developing deep learning architectures embedded with the principles of gene regulation and we will be leveraging millions of existing whole genome measurements of gene activity to learn a mechanistic model of gene regulation in human cells.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Efficiently representing genetic diversity from thousands of microbiome samples</title>
      <link>/2019/09/project-efficiently-representing-genetic-diversity-from-thousands-of-microbiome-samples/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/project-efficiently-representing-genetic-diversity-from-thousands-of-microbiome-samples/</guid>
      <description>&lt;p&gt;The microbiome comprises a heterogeneous mix of bacterial strains, many with strong association to human diseases. Recent work has shown that even the same bacteria could have differences in their genomes across multiple individuals. Such differences, termed structural variations, are strongly associated with host disease risk factors [1]. However, methods for their systematic extraction and profiling are currently lacking. This project aims to make cross-sample analysis of structural variants from hundreds of individual microbiomes feasible by efficient representation of metagenomic data. The colored De-Bruijn graph (cDBG) data structure is a natural choice for this representation [2]. However, current cDBG implementations are either fast at the cost of a large space, or highly space efficient but either slow or lacking valuable practical features.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Integrated Traffic-Communication Simulator for COSMOS testbed</title>
      <link>/2019/09/project-integrated-traffic-communication-simulator-for-cosmos-testbed/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/project-integrated-traffic-communication-simulator-for-cosmos-testbed/</guid>
      <description>&lt;p&gt;Vehicle-to-Vehicle (V2V) has received increasing attention with the development of autonomous driving technology. It is believed that multi-vehicular and multi-informative algorithm is the direction of the autonomous driving technology. However, the stability and liability of the communication prevents the future from extensively embracing V2V-based transportation. Rigorous test is required before V2V can actually hit the road.  Compared with the costly field test, simulation tests are more economical and feasible.  To simulate the V2V communication and evaluate the robustness of current V2V-based algorithm, we are therefore developing a simulation platform integrating different commercial software like SUMO, Veins and OMNET++. These software simulate on the actual New York map, and simulate the vehicular communication in different scenarios and platoon configurations.  Our next step is to use this platform to test our own V2V-based algorithms. The output of this research will eventually provide an open platform which would automatically evaluate personally designed algorithm with least manual work.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Learning the most crucial features of the galaxy merger histories</title>
      <link>/2019/09/project-learning-the-most-crucial-features-of-the-galaxy-merger-histories/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/project-learning-the-most-crucial-features-of-the-galaxy-merger-histories/</guid>
      <description>&lt;p&gt;Galaxies in our universe form hierarchically, continuously merging and absorbing smaller galaxies over cosmic time. In this project we aim to identify the most important features of, as well as generate efficient new features from, the merger histories of galaxies. Namely, features that predict (or physically speaking, determine) the properties of galaxies, e.g. their shape or color. This will be done using the results from a large cosmological simulation, IllustrisTNG (www.tng-project.org). We will begin with identifying ways to represent the rich information in the merger history. We will then compare various ML methods oriented towards feature selection or importance analysis: random forests or gradient boosted trees, L1SVM, neural networks (through analysis of e.g. saliency maps). More advanced models can also be applied, such as neural network models designed for feature selection. Finally, we wish to apply / develop methods that can build &amp;lsquo;interpretable&amp;rsquo; new features by constructing them as algebraic formulas from original input features (inspired by e.g. &lt;a href=&#34;https://science.sciencemag.org/content/324/5923/81&#34;&gt;https://science.sciencemag.org/content/324/5923/81&lt;/a&gt;). The overarching goal is to understand better what in the merger history is most crucial in determining a galaxy&amp;rsquo;s present-day properties, an answer to which can be widely applicable to problems in galaxy formation.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Measuring Liberal Arts: Creating an Index for Higher Education</title>
      <link>/2019/09/project-measuring-liberal-arts-creating-an-index-for-higher-education/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/project-measuring-liberal-arts-creating-an-index-for-higher-education/</guid>
      <description>&lt;p&gt;This project works with a novel corpus of text-based school data to develop a multi-dimensional measure of the degree to which American colleges and universities offer a liberal arts education. We seek a data scientist for various tasks on a project that uses analysis of multiple text corpora to better understand the liberal arts. This is an ongoing three-year project with opportunities for future collaborations, academic publications, and developing and improving existing data science and machine learning skills. Tasks likely include: (1) Using Amazon Web Services to create and maintain cloud-based storage (SQL, S3 buckets) of the project&amp;rsquo;s expanding library of data. (2) Extracting information (named entities, times, places, books, and so on) from millions of plain-text syllabus records. (3) Merging multiple forms of data into a single dataset. (4) Scraping websites for relevant information (e.g., college course offerings, school rankings). Some pages may include dynamically created content that requires the use of a program such as Selenium.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Single-Cell Transcriptome Profiling in Atherosclerosis</title>
      <link>/2019/09/project-single-cell-transcriptome-profiling-in-atherosclerosis/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/project-single-cell-transcriptome-profiling-in-atherosclerosis/</guid>
      <description>&lt;p&gt;Atherosclerosis—a chronic inflammatory disease of the artery wall—is the underlying cause of human coronary heart diseases. Cells within atherosclerotic lesions are heterogeneous and dynamic. Their pathological features have been characterized by histology and flow cytometry and more recently, by bulk-tissue omics profiling. Despite this progress, our knowledge of cell types and their roles in atherogenesis remains incomplete because of masking of differences across cells when using genomic measurement at bulk level. Single-cell RNA sequencing (scRNA-seq) has catalyzed a revolution in understanding of cellular heterogeneity in organ systems and diseases. This project applies scRNA-seq to define the genetic influences on cell subpopulations and functions in atherosclerotic lesion of transgenic mice for candidate risk genes of human coronary heart diseases as inspired by human genomic discoveries. The students involved in this project are expected to work on: (1) analysis of scRNA-seq data using R/Bioconductor packages; (2) Interpretation of the data using pathway and network analysis. Some relevant workflows are available through the &amp;ldquo;Resources&amp;rdquo; page of our lab website at &lt;a href=&#34;https://hanruizhang.github.io/zhanglab/&#34;&gt;https://hanruizhang.github.io/zhanglab/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Waymo &amp; Lyft Driverless Car Data Analysis and Driving Modeling</title>
      <link>/2019/09/project-waymo-lyft-driverless-car-data-analysis-and-driving-modeling/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/project-waymo-lyft-driverless-car-data-analysis-and-driving-modeling/</guid>
      <description>&lt;p&gt;Autonomous driving is developing rapidly. A lot of breakthroughs of autonomous driving have emerged in both academy and industry. However, many traffic accidents related to autonomous driving also occur and cause people’s concern on the safety issue of AV. To ensure safety and reliability, rigorous test and simulation is required before AV can really drive on road. For AV test and simulation, realistic data is an essential component. Comprehensive, multi-regime and sufficient self-driving data would definitely help the AV development.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: A Data-driven Approach for Improving the User Experience of Internet Users</title>
      <link>/2019/01/project-a-data-driven-approach-for-improving-the-user-experience-of-internet-users/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/project-a-data-driven-approach-for-improving-the-user-experience-of-internet-users/</guid>
      <description>&lt;p&gt;Our lives are heavily reliant on Internet-connected devices and services. However, to deliver the desired user experience over the Internet, network operators need to detect and diagnose various network events (e.g., disruption, outage, misconfiguration, etc.) as well as resolve them in real-time. We have developed an Internet-wide measurement infrastructure that collects performance metrics (e.g., latency, jitter, throughput, packet loss rate, signal strength, etc.) from vantage points deployed by real users (mobile phones, WiFi access points, etc.) at regular intervals.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Analysis and Prediction of Opioid Outbreak Clusters</title>
      <link>/2019/01/project-analysis-and-prediction-of-opioid-outbreak-clusters/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/project-analysis-and-prediction-of-opioid-outbreak-clusters/</guid>
      <description>&lt;p&gt;We are interested in investigating how deaths and hospitalizations resulting from opioid overdoses cluster across space and time in the US. This analysis will be conducted with the aid of two comprehensive databases: 1) detailed mortality data across the US; and 2) a stratified sample of all hospitalizations in the US, which can be subset to select for opioid overdoses. Analyses will be extended to drug type (prescription drugs, fentanyl etc.) and subject demographics (age, race, etc.). We have previously conducted similar cluster analysis for other health phenomena.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: ArXivLab: A Platform for Developing and Evaluating Exploratory Tools for the Scientific Literature</title>
      <link>/2019/01/project-arxivlab-a-platform-for-developing-and-evaluating-exploratory-tools-for-the-scientific-literature/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/project-arxivlab-a-platform-for-developing-and-evaluating-exploratory-tools-for-the-scientific-literature/</guid>
      <description>&lt;p&gt;Through ArXivLab we aim to develop the next generation recommender systems for the scientific literature using statistical machine learning approaches. In collaboration with ArXiv we are currently developing a new scholarly literature browser which will be able to extract knowledge implicit in the mathematical and scientific literature, offer advanced mathematical search capabilities and provide personalized recommendations.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Distance Metric Learning in Hyperbolic Spaces</title>
      <link>/2019/01/project-distance-metric-learning-in-hyperbolic-spaces/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/project-distance-metric-learning-in-hyperbolic-spaces/</guid>
      <description>&lt;p&gt;Effective representations and analyses of symbolic data, such as lexical data (words) and networks (graphs), have become of great interest in recent years, due both to advancements in data collection in Natural Language Processing (NLP), and the ubiquity of social networks. Such data often has no natural numerical representation, and is typically described in terms relational expressions or as pairwise similarities. It turns out that finding numerical representations of such data in “Hyperbolic” spaces&amp;mdash;rather than into the more familiar Euclidean spaces&amp;mdash;is a more effective way to preserve valuable relational information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Enhancing Self-Directed Learning Opportunities Using Learning Analytics</title>
      <link>/2019/01/project-enhancing-self-directed-learning-opportunities/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/project-enhancing-self-directed-learning-opportunities/</guid>
      <description>&lt;p&gt;Analyze data from one or more of the following Library Applications/Systems and create visualizations that highlight the most important findings related to our goal of supporting self-directed learning.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Measuring Broadband</title>
      <link>/2019/01/project-measuring-broadband/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/project-measuring-broadband/</guid>
      <description>&lt;p&gt;The Federal Communications Commission (FCC) and the Census regularly publish data on U.S. Internet availability, performance and use, at granularities from census block to county and state. The project goal is to answer questions based on the available data, such as &amp;ldquo;How reliable is Internet access?&amp;rdquo;, &amp;ldquo;Who is deploying fiber where?&amp;rdquo;, &amp;ldquo;Can we predict reliability of different technologies?&amp;rdquo;, &amp;ldquo;Can we predict the deployment of fiber?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Random Forest vs. Neural Networks for Estimating the Ocean Carbon Sink</title>
      <link>/2019/01/random-forest-vs-neural-networks-for-estimating-the-ocean-carbon-sink/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/random-forest-vs-neural-networks-for-estimating-the-ocean-carbon-sink/</guid>
      <description>&lt;p&gt;The ocean has absorbed the equivalent of 41% of industrial-age fossil carbon emissions. In the future, this rate of this ocean carbon sink will determine how much of mankind’s emissions remain in the atmosphere and drive climate change.  To quantify the ocean carbon sink, surface ocean pCO2 must be known, but cannot be measured from satellite; instead it requires direct sampling across the vast and dangerous oceans. Thus, there will never be enough observations to directly estimate the carbon sink as it evolves. Data science can fill this gap by offering robust approaches to extrapolate from sparse observations to full coverage fields given auxiliary data that can be measured remotely.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Blockchain Anomaly Detection</title>
      <link>/2019/01/project-blockchain-anomaly-detection/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/project-blockchain-anomaly-detection/</guid>
      <description>&lt;p&gt;The project has collected a large set of data (&amp;gt;200GB) from a cryptocurrency block chain.  It is developing methods for detecting anomalies in transactions based on newer Social Networks, Graph Analysis and Machine Learning methods. The work involves data cleaning/wrangling and creation and implementation of various algorithms and analyzing the transactions for identifying different set of anomalies and manipulations.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Learning Representations and Patterns in Mathematical Proofs </title>
      <link>/2019/01/project-learning-representations-and-patterns-in-mathematical-proofs/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/project-learning-representations-and-patterns-in-mathematical-proofs/</guid>
      <description>&lt;p&gt;A common challenge for students in heavy proof-based courses is to come up with a long sequence of logical arguments from the problem statement to the final solution. In doing so, they can often skip steps leading to logical leaps or downright incorrect solutions. Ideally the instructor should identify these mis-steps and help students master such proof-based course material. Here we want to take a data-driven approach to address this challenge.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Looking for the Weird in TESS</title>
      <link>/2019/01/project-looking-for-the-weird-in-tess/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/project-looking-for-the-weird-in-tess/</guid>
      <description>&lt;p&gt;Last year one of my graduate students developed a novel algorithm for detecting &amp;ldquo;weird&amp;rdquo; signals in photometric time series, such as those taken by NASA&amp;rsquo;s Kepler Mission and now TESS. An undergraduate students will work in my team to run the algorithm on TESS data, which is just starting to be released publicly (&lt;a href=&#34;https://heasarc.gsfc.nasa.gov/docs/tess/status.html&#34;&gt;https://heasarc.gsfc.nasa.gov/docs/tess/status.html&lt;/a&gt;). We hope to detect strange signatures, possibly including analogs to Tabby&amp;rsquo;s Star, interacting binaries and perhaps even technosignatures.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Measuring Liberal Arts: Creating an Index for Higher Education</title>
      <link>/2019/01/project-measuring-liberal-arts-creating-an-index-for-higher-education/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/project-measuring-liberal-arts-creating-an-index-for-higher-education/</guid>
      <description>&lt;p&gt;This project works with a novel corpus of text-based school data to develop a multi-dimensional measure of the degree to which American colleges and universities offer a liberal arts education. We seek a data scientist for various tasks on a project that uses analysis of multiple text corpora to better understand the liberal arts. This is an ongoing three-year project with opportunities for future collaborations, academic publications, and developing and improving existing data science and machine learning skills.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Quantifying Global Risks</title>
      <link>/2019/01/project-quantifying-global-risks/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/project-quantifying-global-risks/</guid>
      <description>&lt;p&gt;In a globalized world we live in today consequences of catastrophic events easily transgress national borders. Whether it’s a natural disaster, a war or an economic crisis it’s likely to spread out and affect all of us. We propose a framework to model global risks that is not bound to any specific model and is a hybrid of human and machine intelligence. The core of this approach is in using Bayesian Nets of causalities constructed by an analyst equipped with text mining and a map of economic, political and business interconnections.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Technology transfer insights from data</title>
      <link>/2018/01/project-technology-transfer-insights-from-data/</link>
      <pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/project-technology-transfer-insights-from-data/</guid>
      <description>&lt;p&gt;CTV’s core mission is to facilitate the transfer of inventions from academic labs to the market for the benefit of society. In a typical year, CTV receives ~400 inventions, completes ~100 licenses and options, and helps form ~20 startups. A good video summary of CTV is here: &lt;a href=&#34;https://vimeo.com/110193999&#34;&gt;https://vimeo.com/110193999&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Data Science and the regulation of financial markets (application closed)</title>
      <link>/2018/01/project-data-science-and-the-regulation-of-financial-markets/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/project-data-science-and-the-regulation-of-financial-markets/</guid>
      <description>&lt;p&gt;The development of computational data science
techniques in natural language processing (NLP) and machine
learning (ML) algorithms to analyze large and complex textual
information opens new avenues to study intricate processes,
such as government regulation of financial markets, at a scale
unimaginable even a few years ago. This project develops scalable
NLP and ML algorithms (classification, clustering and
ranking methods) that automatically classify laws into various
codes/labels, rank feature sets based on use case, and induce
best structured representation of sentences for various types of
computational analysis.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Enhancing self-directed learning opportunities</title>
      <link>/2018/01/project-enhancing-self-directed-learning-opportunities/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/project-enhancing-self-directed-learning-opportunities/</guid>
      <description>&lt;p&gt;Analyze data from one of the following library applications/systems and create visualizations that highlight the most important findings pertaining to the support of self-directed learning:  Vialogues (TC Video Discussion Application), PocketKnowledge (TC Online Archive), DocDel (E-Reserve System), Pressible (Blogging Platform), Library Website and Mobile App.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Genomic and environmental predictor of preterm birth</title>
      <link>/2018/01/project-genomic-and-environmental-predictor-of-preterm-birth/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/project-genomic-and-environmental-predictor-of-preterm-birth/</guid>
      <description>&lt;p&gt;Predicting preterm birth in nulliparous women is challenging and our efforts to develop predictors for that condition from environmental variables produce insufficient classifier accuracy. Recent studies highlight the involvement of common genetic variants in length of pregnancy. This project involves the development of a risk score for preterm birth based on both genetic and environmental attributes.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Global Interconnections Project (application closed)</title>
      <link>/2018/01/project-global-interconnections-project-application-closed/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/project-global-interconnections-project-application-closed/</guid>
      <description>&lt;p&gt;Understand interconnected nature of global multi-national companies via their supply chain, product and services competition, co-investments and co-ownerships as well as other dependencies between operations and revenue streams. We would like to consider the way news on any company specifically propagate down the connection graph and impact other businesses that are related in a way that is not necessarily explicit.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Modeling Genomic Evolution with Machine Learning</title>
      <link>/2018/01/project-modeling-genomic-evolution-with-machine-learning/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/project-modeling-genomic-evolution-with-machine-learning/</guid>
      <description>&lt;p&gt;A &lt;strong&gt;Fall 2018&lt;/strong&gt; internship is available in the Eaton lab to work on the development and application of machine learning approaches to historical evolutionary inference. Research will involve learning to use high performance distributed computing infrastructure, performing population genetic simulations, fitting machine learning models, and writing reproducible shareable code. The ideal candidate will have experience and interest in Python coding and a reasonable understanding of linear algebra.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Real-time brain state classification</title>
      <link>/2018/01/project-real-time-brain-state-classification/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/project-real-time-brain-state-classification/</guid>
      <description>&lt;p&gt;Using machine learning to conduct brain state classification at real-time on EEG/fNIRS/fMRI data.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Analysis pipeline for strain-level microbiome shotgun sequencing</title>
      <link>/2018/01/project-analysis-pipeline-for-strain-level-microbiome-shotgun-sequencing/</link>
      <pubDate>Sun, 21 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/project-analysis-pipeline-for-strain-level-microbiome-shotgun-sequencing/</guid>
      <description>&lt;p&gt;DNA sequence reads from a community of microbial genomes are currently processed without considering sequence variants. The project involves building a processing pipeline of such billions of short reads, identifying closest strains they might belong to, assembling them into specific clones, calling their variants, and analyzing the dynamic nature of these bacterial strains along sampling points.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Healthcare data analytics internship</title>
      <link>/2018/01/project-healthcare-data-analytics-internship/</link>
      <pubDate>Sun, 21 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/project-healthcare-data-analytics-internship/</guid>
      <description>&lt;p&gt;Recently Columbia University, Cornell, and NewYork-Presbyterian have agreed to integrate their clinical (healthcare) and business IT systems onto one shared platform called Epic. The motivating factors to move to Epic are to enhance the patient experience, improve and integrate care, and give our physicians an integrated technology platform that supports the mission of an academic medical center. The intern will assist with developing the “operational” analytics capabilities of Columbia University Medical Center including financial, healthcare operations and healthcare quality analytics.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Project: Visual-tactile geometric reasoning</title>
      <link>/2018/01/project-visual-tactile-geometric-reasoning/</link>
      <pubDate>Sun, 21 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/project-visual-tactile-geometric-reasoning/</guid>
      <description>&lt;p&gt;Robotic grasp planning based on raw sensory data is difficult due to occlusion and incomplete scene geometry.  Often one sensory modality does not provide enough context to enable reliable planning.  A single depth sensor image cannot provide information about occluded regions of an object, and tactile information is incredibly sparse spatially.  We are building a Deep Learning CNN that combines both 3D vision and tactile information to perform shape completion of an object seen from a single view only, and plan stable grasps on these completed models.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>